{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdKPz7ONoIix"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Input\n",
    "from keras.applications import VGG19\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense, PReLU, Flatten\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYzk684YoAsW"
   },
   "outputs": [],
   "source": [
    "def residual_block(x):\n",
    "    \n",
    "    res = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(x)\n",
    "    res = Activation(activation = \"relu\")(res)\n",
    "    res = BatchNormalization(momentum = 0.8)(res)\n",
    "  \n",
    "    res = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(x)\n",
    "    res = BatchNormalization(momentum = 0.8)(res)\n",
    "  \n",
    "    res = Add()([res, x])\n",
    "  \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFrdYoFprRQ4"
   },
   "outputs": [],
   "source": [
    "def build_gen():\n",
    "  \n",
    "    res_blocks = 16\n",
    "    input_shape = (64, 64, 3)\n",
    "  \n",
    "    input_layer = Input(shape = input_shape)\n",
    "  \n",
    "    gen1 = Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = 'same', activation = 'relu')(input_layer)\n",
    "  \n",
    "    res = residual_block(gen1)\n",
    "    for i in range(res_block - 1):\n",
    "        res = residual_block(res)\n",
    "    \n",
    "    gen2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(res)\n",
    "    gen2 = BatchNormalization(momentum = 0.8)(gen2)\n",
    "  \n",
    "    gen3 = Add()([gen2, gen1])\n",
    "  \n",
    "    gen4 = UpSampling2D(size = (2, 2))(gen3)\n",
    "    gen4 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = \"same\")(gen4)\n",
    "    gen4 = Activation('relu')(gen4)\n",
    "  \n",
    "    gen5 = UpSampling2D(size = (2, 2))(gen4)\n",
    "    gen5 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = \"same\")(gen5)\n",
    "    gen5 = Activation('relu')(gen5)\n",
    "  \n",
    "    gen6 = Conv2D(filters = 3, kernel_size = 9, strides = 1, padding = \"same\")(gen5)\n",
    "    output = Activation('tanh')(gen6)\n",
    "  \n",
    "    model = Model(inputs = [input_layer], outputs = [output], name = 'generator')\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO5P4_Z9t7Uz"
   },
   "outputs": [],
   "source": [
    "def build_disc():\n",
    "    \n",
    "    input_shape = (256, 256, 3)\n",
    "    \n",
    "    input_layer = Input(shape = input_shape)\n",
    "    \n",
    "    disc1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
    "    disc1 = LeakyReLU(alpha = 0.2)(disc1)\n",
    "    \n",
    "    disc2 = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(disc1)\n",
    "    disc2 = LeakyReLU(alpha = 0.2)(disc2)\n",
    "    disc2 = BatchNormalization(momentum = 0.8)(disc2)\n",
    "    \n",
    "    disc3 = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = 'same')(disc2)\n",
    "    disc3 = LeakyReLU(alpha = 0.2)(disc3)\n",
    "    disc3 = BatchNormalization(momentum = 0.8)(disc3)\n",
    "    \n",
    "    disc4 = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same')(disc3)\n",
    "    disc4 = LeakyReLU(alpha = 0.2)(disc4)\n",
    "    disc4 = BatchNormalization(momentum = 0.8)(disc4)\n",
    "    \n",
    "    disc5 = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same')(disc4)\n",
    "    disc5 = LeakyReLU(alpha = 0.2)(disc5)\n",
    "    disc5 = BatchNormalization(momentum = 0.8)(disc5)\n",
    "    \n",
    "    disc6 = Conv2D(filters = 256, kernel_size = 3, strides = 2, padding = 'same')(disc5)\n",
    "    disc6 = LeakyReLU(alpha = 0.2)(disc6)\n",
    "    disc6 = BatchNormalization(momentum = 0.8)(disc6)\n",
    "    \n",
    "    disc7 = Conv2D(filters = 512, kernel_size = 3, strides = 1, padding = 'same')(disc6)\n",
    "    disc7 = LeakyReLU(alpha = 0.2)(disc7)\n",
    "    disc7 = BatchNormalization(momentum = 0.8)(disc7)\n",
    "    \n",
    "    disc8 = Conv2D(filters = 512, kernel_size = 3, strides = 2, padding = 'same')(disc7)\n",
    "    disc8 = LeakyReLU(alpha = 0.2)(disc8)\n",
    "    disc8 = BatchNormalization(momentum = 0.8)(disc8)\n",
    "    \n",
    "    disc9 = Dense(units = 1024)(disc8)\n",
    "    disc9 = LeakyReLU(alpha = 0.2)(disc9)\n",
    "    \n",
    "    output = Dense(units = 1, activation = sigmoid)(disc9)\n",
    "    \n",
    "    model = Model(inputs = [input_layer], outputs = [output], name = 'discriminator')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg():\n",
    "    \n",
    "    input_shape = (256, 256, 3)\n",
    "    \n",
    "    vgg = VGG19(weights = \"imagenet\")\n",
    "    vgg.outputs = [vgg.layers[9].output]\n",
    "    \n",
    "    input_layer = Input(shape = input_shape)\n",
    "    features = vgg(input_layer)\n",
    "    \n",
    "    model = Model(inputs = [input_layer], outputs = [features])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adversarial(generator, discriminator, vgg):\n",
    "    \n",
    "    input_low_resolution = (64, 64, 3)\n",
    "    \n",
    "    fake_hr_images = generator(input_low_resolution)\n",
    "    fake_features = vgg(fake_hr_images)\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    output = discriminator(fake_hr_images)\n",
    "    \n",
    "    model = Model(inputs = [input_low_resolution], outputs = [output, fake_features])\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        print(layer.name, layer.trainable)\n",
    "        \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(data_dir, batch_size, high_resolution_shape, low_resolution_shape):\n",
    "    \n",
    "    print(\"Loading Data\")\n",
    "    \n",
    "    all_images = glob.glob(data_dir)\n",
    "    \n",
    "    images_batch = np.random.choice(all_images, size = batch_size)\n",
    "    \n",
    "    low_resolution_images = []\n",
    "    high_resolution_images = []\n",
    "    \n",
    "    for img in images_batch:\n",
    "        \n",
    "        img1 = imread(img, mode = 'RGB')\n",
    "        img1 = img1.astype(np.float32)\n",
    "        \n",
    "        img1_high_resolution = imresize(img1, high_resolution_shape)\n",
    "        img1_low_resolution = imresize(img1, low_resolution_shape)\n",
    "        \n",
    "        if np.random.random() < 0.5:\n",
    "            \n",
    "            img1_high_resolution = np.flip(img1_high_resolution)\n",
    "            img1_low_resolution = np.flip(img1_low_resolution)\n",
    "            \n",
    "        high_resolution_images.append(img1_high_resolution)\n",
    "        low_resolution_images.append(img1_low_resolution)\n",
    "        \n",
    "        print(\"Data Loaded\")\n",
    "        \n",
    "        return np.asarray(high_resolution_images), np.asarray(low_resolution_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, name, value, batch_no):\n",
    "    \n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    data_dir = \"img_align_celeba\"\n",
    "    epochs = 20000\n",
    "    batch_size = 1\n",
    "    \n",
    "    low_resolution_shape = (64, 64, 3)\n",
    "    high_resolution_shape = (256, 256, 3)\n",
    "    \n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    \n",
    "    vgg = build_vgg()\n",
    "    vgg.trainable = False\n",
    "    vgg.compile(loss = 'mse', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    discriminator = build_disc()\n",
    "    discriminator.compile(loss = 'mse', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    generator = build_generator()\n",
    "    \n",
    "    input_high_resolution = Input(shape = high_resolution_shape)\n",
    "    input_low_resolution = Input(shape = low_resolution_shape)\n",
    "    \n",
    "    generated_high_resolution_images = generator(input_low_resolution)\n",
    "    features = vgg(generated_high_resolution_images)\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    probs = discriminator(generated_high_resolution_images)\n",
    "    \n",
    "    adversarial_model = Model(inputs = [input_low_resolution, input_high_resolution], outputs = [probs, features])\n",
    "    adversarial_model.compile(loss = ['binary_crossentropy', 'mse'], loss_weights = [1e-3, 1], optimizer = optimizer)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir = \"logs/\".format(time.time()))\n",
    "    tensorboard.set_model(generator)\n",
    "    tensorboard.set_model(discriminator)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        \n",
    "        high_resolution_images, low_resolution_images = sample_images(data_dir = data_dir, batch_size = batch_size,\n",
    "                                                                      low_resolution_shape = low_resolution_shape, \n",
    "                                                                      high_resolution_shape = high_resolution_shape)\n",
    "        \n",
    "        high_resolution_images = high_resolution_images/127.5 - 1\n",
    "        low_resolution_images = low_resolution_images/127.5 - 1\n",
    "        \n",
    "        generated_high_resolution_images = generator.predict(low_resolution_images)\n",
    "        \n",
    "        real_labels = np.ones((batch_size, 16, 16, 1))\n",
    "        fake_labels = np.zeros((batch_size, 16, 16, 1))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Photo_Realistic_Image_Generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
